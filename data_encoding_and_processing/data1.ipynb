{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AA', '39.48', '6/11/2007', '9:36am', '-0.18', '181800']\n",
      "['AIG', '71.38', '6/11/2007', '9:36am', '-0.15', '195500']\n",
      "['AXP', '62.58', '6/11/2007', '9:36am', '-0.46', '935000']\n",
      "['BA', '98.31', '6/11/2007', '9:36am', '+0.12', '104800']\n",
      "['C', '53.08', '6/11/2007', '9:36am', '-0.25', '360900']\n",
      "['CAT', '78.29', '6/11/2007', '9:36am', '-0.23', '225400']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headers = next(f_csv)\n",
    "    for row in f_csv:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Symbol='AA', Price='39.48', Date='6/11/2007', Time='9:36am', Change='-0.18', Volume='181800')\n",
      "Row(Symbol='AIG', Price='71.38', Date='6/11/2007', Time='9:36am', Change='-0.15', Volume='195500')\n",
      "Row(Symbol='AXP', Price='62.58', Date='6/11/2007', Time='9:36am', Change='-0.46', Volume='935000')\n",
      "Row(Symbol='BA', Price='98.31', Date='6/11/2007', Time='9:36am', Change='+0.12', Volume='104800')\n",
      "Row(Symbol='C', Price='53.08', Date='6/11/2007', Time='9:36am', Change='-0.25', Volume='360900')\n",
      "Row(Symbol='CAT', Price='78.29', Date='6/11/2007', Time='9:36am', Change='-0.23', Volume='225400')\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headings = next(f_csv)\n",
    "    Row = namedtuple('Row', headings)\n",
    "    for r in f_csv:\n",
    "        row = Row(*r)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Symbol': 'AA', 'Price': '39.48', 'Date': '6/11/2007', 'Time': '9:36am', 'Change': '-0.18', 'Volume': '181800'}\n",
      "{'Symbol': 'AIG', 'Price': '71.38', 'Date': '6/11/2007', 'Time': '9:36am', 'Change': '-0.15', 'Volume': '195500'}\n",
      "{'Symbol': 'AXP', 'Price': '62.58', 'Date': '6/11/2007', 'Time': '9:36am', 'Change': '-0.46', 'Volume': '935000'}\n",
      "{'Symbol': 'BA', 'Price': '98.31', 'Date': '6/11/2007', 'Time': '9:36am', 'Change': '+0.12', 'Volume': '104800'}\n",
      "{'Symbol': 'C', 'Price': '53.08', 'Date': '6/11/2007', 'Time': '9:36am', 'Change': '-0.25', 'Volume': '360900'}\n",
      "{'Symbol': 'CAT', 'Price': '78.29', 'Date': '6/11/2007', 'Time': '9:36am', 'Change': '-0.23', 'Volume': '225400'}\n"
     ]
    }
   ],
   "source": [
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.DictReader(f)\n",
    "    for row in f_csv:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['Symbol','Price','Date','Time','Change','Volume']\n",
    "rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800),\n",
    "        ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500),\n",
    "        ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000)]\n",
    "\n",
    "with open('stocks2.csv', 'w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerow(headers)\n",
    "    f_csv.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']\n",
    "rows = [{'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.18, 'Volume':181800},\n",
    "        {'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500},\n",
    "        {'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000}]\n",
    "\n",
    "with open('stocks3.csv','w') as f:\n",
    "    f_csv = csv.DictWriter(f, headers)\n",
    "    f_csv.writeheader()\n",
    "    f_csv.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800)\n",
      "('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500)\n",
      "('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000)\n",
      "('BA', 98.31, '6/11/2007', '9:36am', 0.12, 104800)\n",
      "('C', 53.08, '6/11/2007', '9:36am', -0.25, 360900)\n",
      "('CAT', 78.29, '6/11/2007', '9:36am', -0.23, 225400)\n"
     ]
    }
   ],
   "source": [
    "col_types = [str, float, str, str, float, int]\n",
    "\n",
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headers = next(f_csv)\n",
    "    for row in f_csv:\n",
    "        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –∫ —ç–ª–µ–º–µ–Ω—Ç–∞–º —Å—Ç—Ä–æ–∫–∏\n",
    "        row = tuple(convert(value) for convert, value in zip(col_types, row))\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading as dicts with type conversion\n",
      "{'Symbol': 'AA', 'Price': 39.48, 'Date': '6/11/2007', 'Time': '9:36am', 'Change': -0.18, 'Volume': 181800}\n",
      "{'Symbol': 'AIG', 'Price': 71.38, 'Date': '6/11/2007', 'Time': '9:36am', 'Change': -0.15, 'Volume': 195500}\n",
      "{'Symbol': 'AXP', 'Price': 62.58, 'Date': '6/11/2007', 'Time': '9:36am', 'Change': -0.46, 'Volume': 935000}\n",
      "{'Symbol': 'BA', 'Price': 98.31, 'Date': '6/11/2007', 'Time': '9:36am', 'Change': 0.12, 'Volume': 104800}\n",
      "{'Symbol': 'C', 'Price': 53.08, 'Date': '6/11/2007', 'Time': '9:36am', 'Change': -0.25, 'Volume': 360900}\n",
      "{'Symbol': 'CAT', 'Price': 78.29, 'Date': '6/11/2007', 'Time': '9:36am', 'Change': -0.23, 'Volume': 225400}\n"
     ]
    }
   ],
   "source": [
    "print('Reading as dicts with type conversion')\n",
    "field_types = [('Price', float), ('Change', float), ('Volume', int)]\n",
    "\n",
    "with open('stocks.csv') as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        row.update((key, conversion(row[key])) for key, conversion in field_types)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'ACME'), ('shares', 50), ('price', 490.1)])\n",
      "ACME\n",
      "50\n",
      "490.1\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import json\n",
    "\n",
    "s = '{\"name\": \"ACME\", \"shares\": 50, \"price\": 490.1}'\n",
    "data = json.loads(s, object_pairs_hook=OrderedDict)\n",
    "print(data)\n",
    "\n",
    "class JSONObject:\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "        \n",
    "data1 = json.loads(s, object_hook=JSONObject)\n",
    "print(data1.name)\n",
    "print(data1.shares)\n",
    "print(data1.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"__classname__\": \"Point\", \"x\": 2, \"y\": 3}\n",
      "<__main__.Point object at 0x0000020F5B5BDA20>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "class Point:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "def serialize_instance(obj):\n",
    "    d = {'__classname__' : type(obj).__name__}\n",
    "    d.update(vars(obj))\n",
    "    return d\n",
    "\n",
    "# –°–ª–æ–≤–∞—Ä—å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–º–µ–Ω –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã\n",
    "classes = {'Point': Point}\n",
    "\n",
    "def unserialize_object(d):\n",
    "    clsname = d.pop('__classname__', None)\n",
    "    if clsname:\n",
    "        cls = classes[clsname]\n",
    "        obj = cls.__new__(cls) # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –±–µ–∑ –≤—ã–∑–æ–≤–∞ __init__\n",
    "        for key, value in d.items():\n",
    "            setattr(obj, key, value)\n",
    "            return obj\n",
    "    else:\n",
    "        return d\n",
    "    \n",
    "p = Point(2, 3)\n",
    "s = json.dumps(p, default=serialize_instance)\n",
    "print(s)\n",
    "\n",
    "a = json.loads(s, object_hook=unserialize_object)\n",
    "print(a)\n",
    "\n",
    "print(a.x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Python: A Close Look at a FastAPI Example Application\n",
      "Mon, 03 Nov 2025 14:00:00 +0000\n",
      "https://realpython.com/fastapi-python-web-apis/\n",
      "\n",
      "Reuven Lerner: Want to learn uv?\n",
      "Mon, 03 Nov 2025 13:15:29 +0000\n",
      "https://lerner.co.il/2025/11/03/want-to-learn-uv/\n",
      "\n",
      "Real Python: Quiz: A Close Look at a FastAPI Example Application\n",
      "Mon, 03 Nov 2025 12:00:00 +0000\n",
      "https://realpython.com/quizzes/fastapi-python-web-apis/\n",
      "\n",
      "eGenix.com: PyDDF Python Herbst Sprint 2025\n",
      "Mon, 03 Nov 2025 09:00:00 +0000\n",
      "https://www.egenix.com/company/news/PyDDF-Herbst-Sprint-2025\n",
      "\n",
      "Hugo van Kemenade: Python Core Sprint 2025\n",
      "Mon, 03 Nov 2025 08:58:00 +0000\n",
      "https://hugovk.dev/blog/2025/python-core-sprint/\n",
      "\n",
      "Python Bytes: #456 You're so wrong\n",
      "Mon, 03 Nov 2025 08:00:00 +0000\n",
      "https://pythonbytes.fm/episodes/show/456/youre-so-wrong\n",
      "\n",
      "Django Weblog: Announcing DjangoCon Europe 2026 in Athens, Greece! üèõÔ∏èüá¨üá∑\n",
      "Mon, 03 Nov 2025 07:00:00 +0000\n",
      "https://www.djangoproject.com/weblog/2025/nov/03/announcing-djangocon-europe-2026-in-athens-greece/\n",
      "\n",
      "Armin Ronacher: Absurd Workflows: Durable Execution With Just Postgres\n",
      "Mon, 03 Nov 2025 00:00:00 +0000\n",
      "https://lucumr.pocoo.org/2025/11/3/absurd-workflows/\n",
      "\n",
      "Stefanie Molin: Becoming a Core Developer\n",
      "Sun, 02 Nov 2025 16:50:00 +0000\n",
      "https://stefaniemolin.com/articles/open-source/becoming-a-core-developer\n",
      "\n",
      "Django Weblog: Five ways to discover Django packages\n",
      "Sun, 02 Nov 2025 14:29:05 +0000\n",
      "https://www.djangoproject.com/weblog/2025/nov/02/five-ways-to-discover-django-packages/\n",
      "\n",
      "Brian Okken: Announcing the Lean TDD book\n",
      "Sun, 02 Nov 2025 00:00:00 +0000\n",
      "https://pythontest.com/announcing-lean-tdd/\n",
      "\n",
      "The Python Coding Stack: And Now You Know Your ABC\n",
      "Sat, 01 Nov 2025 12:52:51 +0000\n",
      "https://www.thepythoncodingstack.com/p/and-now-you-know-your-abc-python-abstract-base-classes\n",
      "\n",
      "Zero to Mastery: [October 2025] Python Monthly Newsletter üêç\n",
      "Sat, 01 Nov 2025 10:00:00 +0000\n",
      "https://zerotomastery.io/blog/python-monthly-newsletter-october-2025/?utm_source=python-rss-feed\n",
      "\n",
      "Talk Python to Me: #526: Building Data Science with Foundation LLM Models\n",
      "Sat, 01 Nov 2025 08:00:00 +0000\n",
      "https://talkpython.fm/episodes/show/526/building-data-science-with-foundation-llm-models\n",
      "\n",
      "Tryton News: Newsletter November 2025\n",
      "Sat, 01 Nov 2025 07:00:38 +0000\n",
      "https://discuss.tryton.org/t/newsletter-november-2025/8920\n",
      "\n",
      "Seth Michael Larson: RSS feed for new Nintendo Classics games\n",
      "Sat, 01 Nov 2025 00:00:00 +0000\n",
      "https://sethmlarson.dev/rss-feed-for-nintendo-classics?utm_campaign=rss\n",
      "\n",
      "Django Weblog: Django Developers Survey 2025 results\n",
      "Fri, 31 Oct 2025 16:47:10 +0000\n",
      "https://www.djangoproject.com/weblog/2025/oct/31/django-developers-survey-2025-results/\n",
      "\n",
      "PyCon: PyCon US 2026 - Call for Proposals Now Open!\n",
      "Fri, 31 Oct 2025 14:02:18 +0000\n",
      "https://pycon.blogspot.com/2025/10/pycon-us-2026-call-for-proposals-now.html\n",
      "\n",
      "Kushal Das: Not anymore a director at the PSF board\n",
      "Fri, 31 Oct 2025 13:00:33 +0000\n",
      "https://kushaldas.in/posts/not-anymore-a-director-at-psf.html\n",
      "\n",
      "Real Python: The Real Python Podcast ‚Äì Episode #272: Michael Kennedy: Managing Your Own Python Infrastructure\n",
      "Fri, 31 Oct 2025 12:00:00 +0000\n",
      "https://realpython.com/podcasts/rpp/272/\n",
      "\n",
      "Python Software Foundation: Announcing Python Software Foundation Fellow Members for Q3 2025! üéâ\n",
      "Fri, 31 Oct 2025 11:37:24 +0000\n",
      "https://pyfound.blogspot.com/2025/10/announcing-python-software-foundation.html\n",
      "\n",
      "Python Software Foundation: Improving security and integrity of Python package archives\n",
      "Thu, 30 Oct 2025 11:04:57 +0000\n",
      "https://pyfound.blogspot.com/2025/10/slippery-zips-and-sticky-tar-pits-security-and-archives-white-paper.html\n",
      "\n",
      "Ned Batchelder: Side project advice\n",
      "Thu, 30 Oct 2025 10:23:13 +0000\n",
      "https://nedbatchelder.com/blog/202510/side_project_advice.html\n",
      "\n",
      "Django Weblog: Django is now a CVE Numbering Authority (CNA)\n",
      "Thu, 30 Oct 2025 08:48:56 +0000\n",
      "https://www.djangoproject.com/weblog/2025/oct/30/django-is-now-a-cve-numbering-authority-cna/\n",
      "\n",
      "Django Weblog: DSF member of the month - Anna Makarudze\n",
      "Thu, 30 Oct 2025 05:00:00 +0000\n",
      "https://www.djangoproject.com/weblog/2025/oct/30/dsf-member-of-the-month-anna-makarudze/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from xml.etree.ElementTree import parse\n",
    "\n",
    "# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –ø–∞—Ä—Å–∏–Ω–≥ RSS-–∫–∞–Ω–∞–ª–∞\n",
    "u = urlopen('http://planet.python.org/rss20.xml')\n",
    "doc = parse(u)\n",
    "\n",
    "# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥ –Ω—É–∂–Ω—ã—Ö —Ç–µ–≥–æ–≤\n",
    "for item in doc.iterfind('channel/item'):\n",
    "    title = item.findtext('title')\n",
    "    date = item.findtext('pubDate')\n",
    "    link = item.findtext('link')\n",
    "    \n",
    "    print(title)\n",
    "    print(date)\n",
    "    print(link)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xml.etree.ElementTree.ElementTree object at 0x0000020F5B3A1240>\n",
      "<Element 'title' at 0x0000020F59016390>\n",
      "title\n",
      "Planet Python\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "print(doc.find('channel/title'))\n",
    "print(doc.find('channel/title').tag)\n",
    "print(doc.find('channel/title').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element 'stock' at 0x0000020F5B71C130>\n",
      "b'<stock><name>GOOG</name><shares>100</shares><price>490.1</price></stock>'\n",
      "b'<stock _id=\"1234\"><name>GOOG</name><shares>100</shares><price>490.1</price></stock>'\n",
      "<item><name><spam></name></item>\n",
      "b'<item><name>&lt;spam&gt;</name></item>'\n"
     ]
    }
   ],
   "source": [
    "from xml.etree.ElementTree import Element, tostring\n",
    "\n",
    "def dict_to_xml(tag, d):\n",
    "    '''–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä –∫–ª—é—á/–∑–Ω–∞—á–µ–Ω–∏–µ –≤ XML'''\n",
    "    elem = Element(tag)\n",
    "    for key, val in d.items():\n",
    "        child = Element(key)\n",
    "        child.text = str(val)\n",
    "        elem.append(child)\n",
    "    return elem\n",
    "\n",
    "s = {'name': 'GOOG', 'shares': 100, 'price':490.1}\n",
    "e = dict_to_xml('stock', s)\n",
    "\n",
    "print(e)\n",
    "print(tostring(e))\n",
    "\n",
    "e.set('_id','1234')\n",
    "print(tostring(e))\n",
    "\n",
    "def dict_to_xml_str(tag, d):\n",
    "    '''–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä –∫–ª—é—á/–∑–Ω–∞—á–µ–Ω–∏–µ –≤ XML'''\n",
    "    parts = [f'<{tag}>']\n",
    "    for key, val in d.items():\n",
    "        parts.append(f'<{key}>{val}</{key}>')\n",
    "        parts.append(f'</{tag}>')\n",
    "    return ''.join(parts)\n",
    "\n",
    "\n",
    "d = {'name': '<spam>'}\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏\n",
    "print(dict_to_xml_str('item', d))\n",
    "\n",
    "# –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ XML\n",
    "e = dict_to_xml('item', d)\n",
    "print(tostring(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element 'stop' at 0x0000020F5B74C090>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from xml.etree.ElementTree import parse, Element\n",
    "\n",
    "doc = parse('pred.xml')\n",
    "root = doc.getroot()\n",
    "print(root)\n",
    "\n",
    "# –£–¥–∞–ª–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n",
    "root.remove(root.find('sri')) \n",
    "root.remove(root.find('cr'))\n",
    "\n",
    "# –í—Å—Ç–∞–≤–∫–∞ –Ω–æ–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ <nm>...</nm>\n",
    "print(list(root).index(root.find('nm')))\n",
    "e = Element('spam')\n",
    "e.text = 'This is a test'\n",
    "root.insert(2, e)\n",
    "\n",
    "# –ó–∞–ø–∏—Å—å –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ñ–∞–π–ª\n",
    "doc.write('newpred.xml', xml_declaration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Beazley\n",
      "<Element 'content' at 0x0000020F5B786200>\n",
      "None\n",
      "<Element '{http://www.w3.org/1999/xhtml}html' at 0x0000020F5B7862F0>\n",
      "None\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "doc = parse('test.xml')\n",
    "\n",
    "# –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç–∞—é—Ç\n",
    "print(doc.findtext('author'))\n",
    "print(doc.find('content'))\n",
    "\n",
    "# –ó–∞–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏–º–µ–Ω (–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)\n",
    "print(doc.find('content/html'))\n",
    "\n",
    "# –†–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∏ –ø–æ–ª–Ω–æ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏\n",
    "print(doc.find('content/{http://www.w3.org/1999/xhtml}html'))\n",
    "\n",
    "# –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç\n",
    "print(doc.findtext('content/{http://www.w3.org/1999/xhtml}html/head/title'))\n",
    "\n",
    "# –ü–æ–ª–Ω–æ—Å—Ç—å—é –æ–ø—Ä–µ–¥–µ–ª–µ–Ω\n",
    "print(doc.findtext('content/{http://www.w3.org/1999/xhtml}html/{http://www.w3.org/1999/xhtml}head/{http://www.w3.org/1999/xhtml}title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element '{http://www.w3.org/1999/xhtml}html' at 0x0000020F5B7862F0>\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "class XMLNamespaces:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.namespaces = {}\n",
    "        for name, uri in kwargs.items():\n",
    "            self.register(name, uri)\n",
    "    \n",
    "    def register(self, name, uri):\n",
    "        self.namespaces[name] = '{'+uri+'}'\n",
    "    \n",
    "    def __call__(self, path):\n",
    "        return path.format_map(self.namespaces)\n",
    "    \n",
    "ns = XMLNamespaces(html='http://www.w3.org/1999/xhtml')\n",
    "print(doc.find(ns('content/{html}html')))\n",
    "print(doc.findtext(ns('content/{html}html/{html}head/{html}title')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end <Element 'author' at 0x0000020F5B7867F0>\n",
      "start-ns ('', 'http://www.w3.org/1999/xhtml')\n",
      "end <Element '{http://www.w3.org/1999/xhtml}title' at 0x0000020F5B7877E0>\n",
      "end <Element '{http://www.w3.org/1999/xhtml}head' at 0x0000020F5B786930>\n",
      "end <Element '{http://www.w3.org/1999/xhtml}h1' at 0x0000020F5B787A10>\n",
      "end <Element '{http://www.w3.org/1999/xhtml}body' at 0x0000020F5B787970>\n",
      "end <Element '{http://www.w3.org/1999/xhtml}html' at 0x0000020F5B7871F0>\n",
      "end-ns None\n",
      "end <Element 'content' at 0x0000020F5B7872E0>\n",
      "end <Element 'top' at 0x0000020F5B34BF10>\n"
     ]
    }
   ],
   "source": [
    "from xml.etree.ElementTree import iterparse\n",
    "\n",
    "for evt, elem in iterparse('test.xml', ('end', 'start-ns', 'end-ns')):\n",
    "    print(evt, elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "stocks = [\n",
    "    ('GOOG', 100, 490.1),\n",
    "    ('AAPL', 50, 545.75),\n",
    "    ('FB', 150, 7.45),\n",
    "    ('HPQ', 75, 33.2),\n",
    "]\n",
    "\n",
    "db = sqlite3.connect('database.db')\n",
    "c = db.cursor()\n",
    "c.execute('create table portfolio (symbol text, shares integer, price real)')\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.executemany('insert into portfolio values (?,?,?)', stocks)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GOOG', 100, 490.1)\n",
      "('AAPL', 50, 545.75)\n",
      "('FB', 150, 7.45)\n",
      "('HPQ', 75, 33.2)\n"
     ]
    }
   ],
   "source": [
    "for row in db.execute('select * from portfolio'):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GOOG', 100, 490.1)\n",
      "('AAPL', 50, 545.75)\n"
     ]
    }
   ],
   "source": [
    "min_price = 100\n",
    "for row in db.execute('select * from portfolio where price >= ?', (min_price,)):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'68656c6c6f'\n",
      "b'hello'\n",
      "b'68656C6C6F'\n",
      "b'hello'\n",
      "68656C6C6F\n",
      "b'aGVsbG8='\n",
      "b'hello'\n"
     ]
    }
   ],
   "source": [
    "import binascii\n",
    "import base64\n",
    "\n",
    "# –ò–∑–Ω–∞—á–∞–ª—å–Ω–∞—è –±–∞–π—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞\n",
    "s = b'hello'\n",
    "\n",
    "# –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –≤ hex\n",
    "h = binascii.b2a_hex(s)\n",
    "print(h)\n",
    "\n",
    "# –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω–æ –≤ –±–∞–π—Ç—ã\n",
    "print(binascii.a2b_hex(h))\n",
    "\n",
    "# base64\n",
    "h = base64.b16encode(s)\n",
    "print(h)\n",
    "print(base64.b16decode(h))\n",
    "print(h.decode('ascii'))\n",
    "\n",
    "# –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –≤ Base64\n",
    "a = base64.b64encode(s)\n",
    "print(a)\n",
    "\n",
    "# –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∏–∑ Base64\n",
    "print(base64.b64decode(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from struct import Struct\n",
    "\n",
    "def write_records(records, format, f):\n",
    "    '''–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–æ—Ä—Ç–µ–∂–µ–π –≤ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∞–π–ª —Å—Ç—Ä—É–∫—Ç—É—Ä.'''\n",
    "    record_struct = Struct(format)\n",
    "    for r in records:\n",
    "        f.write(record_struct.pack(*r))\n",
    "        \n",
    "records = [(1, 2.3, 4.5),\n",
    "           (6, 7.8, 9.0),\n",
    "           (12, 13.4, 56.7)]\n",
    "\n",
    "with open('data.b', 'wb') as f:\n",
    "    write_records(records, '<idd', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2.3, 4.5)\n",
      "(6, 7.8, 9.0)\n",
      "(12, 13.4, 56.7)\n"
     ]
    }
   ],
   "source": [
    "def read_records(format, f):\n",
    "    record_struct = Struct(format)\n",
    "    chunks = iter(lambda: f.read(record_struct.size), b'')\n",
    "    return (record_struct.unpack(chunk) for chunk in chunks)\n",
    "\n",
    "with open('data.b', 'rb') as f:\n",
    "    for rec in read_records('<idd', f):\n",
    "        print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2.3, 4.5)\n",
      "(6, 7.8, 9.0)\n",
      "(12, 13.4, 56.7)\n"
     ]
    }
   ],
   "source": [
    "def unpack_records(format, data):\n",
    "    record_struct = Struct(format)\n",
    "    return (record_struct.unpack_from(data, offset) for offset in range(0, len(data), record_struct.size))\n",
    "\n",
    "with open('data.b', 'rb') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "for rec in unpack_records('<idd', data):\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x08@'\n",
      "(1, 2.0, 3.0)\n"
     ]
    }
   ],
   "source": [
    "record_struct = Struct('<idd')\n",
    "print(record_struct.size)\n",
    "print(record_struct.pack(1, 2.0, 3.0))\n",
    "print(record_struct.unpack(record_struct.pack(1, 2.0, 3.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x000001B3F3158070>\n",
      "b'\\x01\\x00\\x00\\x00ffffff\\x02@\\x00\\x00\\x00\\x00\\x00\\x00\\x12@'\n",
      "b'\\x06\\x00\\x00\\x00333333\\x1f@\\x00\\x00\\x00\\x00\\x00\\x00\"@'\n",
      "b'\\x0c\\x00\\x00\\x00\\xcd\\xcc\\xcc\\xcc\\xcc\\xcc*@\\x9a\\x99\\x99\\x99\\x99YL@'\n"
     ]
    }
   ],
   "source": [
    "with open('data.b', 'rb') as f:\n",
    "    chunks = iter(lambda: f.read(20), b'')\n",
    "    print(chunks)\n",
    "\n",
    "    for chk in chunks:\n",
    "        print(chk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.3 4.5\n",
      "6 7.8 9.0\n",
      "12 13.4 56.7\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "def read_records(format, f):\n",
    "    record_struct = Struct(format)\n",
    "    while True:\n",
    "        chk = f.read(record_struct.size)\n",
    "        if chk == b'':\n",
    "            break\n",
    "        yield record_struct.unpack(chk)\n",
    "    return records\n",
    "\n",
    "def unpack_records(format, data):\n",
    "    record_struct = Struct(format)\n",
    "    return (record_struct.unpack(data[offset:offset + record_struct.size]) for offset in range(0, len(data), record_struct.size))\n",
    "\n",
    "Record = namedtuple('Record', ['kind','x','y'])\n",
    "\n",
    "with open('data.b', 'rb') as f:\n",
    "    records = (Record(*r) for r in read_records('<idd', f))\n",
    "    \n",
    "    for r in records:\n",
    "        print(r.kind, r.x, r.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[( 1,  2.3,  4.5) ( 6,  7.8,  9. ) (12, 13.4, 56.7)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('data.b', 'rb') as f:\n",
    "    records = np.fromfile(f, dtype='<i,<d,<d')\n",
    "    print(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import itertools\n",
    "\n",
    "\n",
    "polys = [\n",
    "    [ (1.0, 2.5), (3.5, 4.0), (2.5, 1.5) ],\n",
    "    [ (7.0, 1.2), (5.1, 3.0), (0.5, 7.5), (0.8, 9.0) ],\n",
    "    [ (3.4, 6.3), (1.2, 0.5), (4.6, 9.2) ],\n",
    "]\n",
    "\n",
    "def write_polys(filename, polys):\n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏–π –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥\n",
    "    flattened = list(itertools.chain(*polys))\n",
    "    min_x = min(x for x, y in flattened)\n",
    "    max_x = max(x for x, y in flattened)\n",
    "    min_y = min(y for x, y in flattened)\n",
    "    max_y = max(y for x, y in flattened)\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(struct.pack('<iddddi', 0x1234, min_x, min_y, max_x, max_y, len(polys)))\n",
    "        \n",
    "        for poly in polys:\n",
    "            size = len(poly) * struct.calcsize('<dd')\n",
    "            f.write(struct.pack('<i', size+4))\n",
    "            for pt in poly:\n",
    "                f.write(struct.pack('<dd', *pt))\n",
    "            \n",
    "# –í—ã–∑—ã–≤–∞–µ–º —Å –Ω–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ–ª–∏–≥–æ–Ω–æ–≤\n",
    "write_polys('polys.bin', polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1.0, 2.5), (3.5, 4.0), (2.5, 1.5)],\n",
       " [(7.0, 1.2), (5.1, 3.0), (0.5, 7.5), (0.8, 9.0)],\n",
       " [(3.4, 6.3), (1.2, 0.5), (4.6, 9.2)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_polys(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # –ß–∏—Ç–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫\n",
    "        header = f.read(40)\n",
    "        file_code, min_x, min_y, max_x, max_y, num_polys = struct.unpack('<iddddi', header)\n",
    "        \n",
    "        polys = []\n",
    "        for n in range(num_polys):\n",
    "            pbytes, = struct.unpack('<i', f.read(4))\n",
    "            poly = []\n",
    "            for m in range(pbytes // 16):\n",
    "                pt = struct.unpack('<dd', f.read(16))\n",
    "                poly.append(pt)\n",
    "            polys.append(poly)\n",
    "    \n",
    "    return polys\n",
    "\n",
    "read_polys('polys.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0.5\n",
      "0.5\n",
      "7.0\n",
      "9.2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "class StructField:\n",
    "    '''–î–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–π –ø—Ä–æ—Å—Ç–æ–µ –ø–æ–ª–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã'''\n",
    "    def __init__(self, format, offset):\n",
    "        self.format = format\n",
    "        self.offset = offset\n",
    "        \n",
    "    def __get__(self, instance, cls):\n",
    "        if instance is None:\n",
    "            return self\n",
    "        else:\n",
    "            r = struct.unpack_from(self.format, instance._buffer, self.offset)\n",
    "        return r[0] if len(r) == 1 else r\n",
    "    \n",
    "\n",
    "class Structure:\n",
    "    def __init__(self, bytedata):\n",
    "        self._buffer = memoryview(bytedata)\n",
    "        \n",
    "class PolyHeader(Structure):\n",
    "    file_code = StructField('<i', 0)\n",
    "    min_x = StructField('<d', 4)\n",
    "    min_y = StructField('<d', 12)\n",
    "    max_x = StructField('<d', 20)\n",
    "    max_y = StructField('<d', 28)\n",
    "    num_polys = StructField('<i', 36)\n",
    "    \n",
    "with open('polys.bin', 'rb') as f:\n",
    "    phead = PolyHeader(f.read(40))\n",
    "    print(phead.file_code == 0x1234)\n",
    "    print(phead.min_x)\n",
    "    print(phead.min_y)\n",
    "    print(phead.max_x)\n",
    "    print(phead.max_y)\n",
    "    print(phead.num_polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0.5\n",
      "0.5\n",
      "7.0\n",
      "9.2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "class StructureMeta(type):\n",
    "    '''–ú–µ—Ç–∞–∫–ª–∞—Å—Å, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ã StructField'''\n",
    "    def __init__(self, clsname, bases, clsdict):\n",
    "        fields = getattr(self, '_fields_', [])\n",
    "        byte_order = ''\n",
    "        offset = 0\n",
    "        \n",
    "        for format, fieldname in fields:\n",
    "            if format.startswith(('<','>','!','@')):\n",
    "                byte_order = format[0]\n",
    "                format = format[1:]\n",
    "            format = byte_order + format\n",
    "            setattr(self, fieldname, StructField(format, offset))\n",
    "            offset += struct.calcsize(format)\n",
    "        setattr(self, 'struct_size', offset)\n",
    "        \n",
    "\n",
    "class Structure(metaclass=StructureMeta):\n",
    "    def __init__(self, bytedata):\n",
    "        self._buffer = bytedata\n",
    "        \n",
    "    @classmethod\n",
    "    def from_file(cls, f):\n",
    "        return cls(f.read(cls.struct_size))\n",
    "    \n",
    "\n",
    "class PolyHeader(Structure):\n",
    "    _fields_ = [\n",
    "        ('<i', 'file_code'),\n",
    "        ('d', 'min_x'),\n",
    "        ('d', 'min_y'),\n",
    "        ('d', 'max_x'),\n",
    "        ('d', 'max_y'),\n",
    "        ('i', 'num_polys')\n",
    "    ]\n",
    "    \n",
    "\n",
    "with open('polys.bin', 'rb') as f:\n",
    "    phead = PolyHeader(f.read(40))\n",
    "    print(phead.file_code == 0x1234)\n",
    "    print(phead.min_x)\n",
    "    print(phead.min_y)\n",
    "    print(phead.max_x)\n",
    "    print(phead.max_y)\n",
    "    print(phead.num_polys)    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[<__main__.SizedRecord object at 0x000001B3FF518700>, <__main__.SizedRecord object at 0x000001B3FF620D00>, <__main__.SizedRecord object at 0x000001B3FF620AF0>]\n",
      "Polygon 0\n",
      "(1.0, 2.5)\n",
      "(3.5, 4.0)\n",
      "(2.5, 1.5)\n",
      "Polygon 1\n",
      "(7.0, 1.2)\n",
      "(5.1, 3.0)\n",
      "(0.5, 7.5)\n",
      "(0.8, 9.0)\n",
      "Polygon 2\n",
      "(3.4, 6.3)\n",
      "(1.2, 0.5)\n",
      "(4.6, 9.2)\n"
     ]
    }
   ],
   "source": [
    "class SizedRecord:\n",
    "    def __init__(self, bytedata):\n",
    "        self._buffer = memoryview(bytedata)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_file(cls, f, size_fmt, includes_size=True):\n",
    "        sz_nbytes = struct.calcsize(size_fmt)\n",
    "        sz_bytes = f.read(sz_nbytes)\n",
    "        sz, = struct.unpack(size_fmt, sz_bytes)\n",
    "        buf = f.read(sz - includes_size * sz_nbytes)\n",
    "        return cls(buf)\n",
    "    \n",
    "    def iter_as(self, code):\n",
    "        if isinstance(code, str):\n",
    "            s = struct.Struct(code)\n",
    "            for off in range(0, len(self._buffer), s.size):\n",
    "                yield s.unpack_from(self._buffer, off)\n",
    "        elif isinstance(code, StructureMeta):\n",
    "            size = code.struct_size\n",
    "            for off in range(0, len(self._buffer), size):\n",
    "                data = self._buffer[off:off+size]\n",
    "                yield code(data)\n",
    "                \n",
    "\n",
    "with open('polys.bin', 'rb') as f:\n",
    "    phead = PolyHeader.from_file(f)\n",
    "    print(phead.num_polys)\n",
    "    \n",
    "    polydata = [SizedRecord.from_file(f, '<i') for n in range(phead.num_polys)]\n",
    "    print(polydata)\n",
    "    \n",
    "    for n, poly in enumerate(polydata):\n",
    "        print('Polygon', n)\n",
    "        for p in poly.iter_as('<dd'):\n",
    "            print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
